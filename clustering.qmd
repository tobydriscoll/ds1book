# Clustering
{{< include _macros.qmd >}}

```{python}
#| code-fold: true
import numpy as np
from numpy.random import default_rng
import pandas as pd
import seaborn as sns
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import confusion_matrix, f1_score, balanced_accuracy_score
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_curve
from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.model_selection import cross_validate, validation_curve
from sklearn.model_selection import GridSearchCV
```

# Clustering

In supervised learning, the data samples are supplied with labels, and the goal of the learner is to generalize the examples to new values. In unsupervised learning, there are no labels. Instead, the goal is to discover structure that is intrinsic to the feature matrix. Common problem types in unsupervised learning are

* **Clustering**. Determine whether the samples roughly divide into a small number of classes.
* **Dimension reduction**. Find a reduced set of features, or create a small set of new features, that describe the data well.
* **Outlier detection**. Find anomalous values in the data set, and remove them or impute replacements.

In this chapter we will look at clustering. In order to get a feeling for the algorithms, we will apply them to three illustrative datasets:

* blobs
: This dataset has one distinct blob, plus two that kind of overlap a bit:
```{python}
#| code-fold: true
from sklearn.datasets import make_blobs
def blobs_data():
    X, y = make_blobs(
        n_samples=[60, 50, 40],
        centers=[[-2,3], [3,1.5], [1,-3]],
        cluster_std=[0.5, 0.9, 1.2],
        random_state = 19716
        )

    return pd.DataFrame({"x1":X[:,0], "x2":X[:,1], "class":y})
```

```{python}
blobs = blobs_data()
sns.relplot(blobs, x="x1", y="x2", hue="class");
```

* stripes 
: In this dataset, there is clear separation along one axis and a continuous blob along the other:
```{python}
#| code-fold: true
def stripes_data():
    rng = default_rng(9)
    x1,x2,cls = [],[],[]
    for i in range(3):
        x1.extend( rng.uniform(-10, 10, size=200) )
        x2.extend( 2.5*i+rng.uniform(0, 1, size=200) )
        cls.extend( [i]*200)
    return pd.DataFrame({"x1": x1, "x2": x2, "class": cls})
```

```{python}
stripes = stripes_data()
sns.relplot(stripes, x="x1", y="x2", hue="class");
```

* bullseye 
: This is the most challenging dataset, because the clusters can't be separated by anything like straight lines:
```{python}
#| code-fold: true
def bullseye_data():
    rng = default_rng(6)
    inner = 0.8*rng.normal(size=(100, 2))
    theta = rng.uniform(0, 2*np.pi, size=200)
    r = rng.uniform(3, 4, size=200)
    middle = np.vstack((r*np.cos(theta), r*np.sin(theta))).T
    r = rng.uniform(5,6,size=200)
    outer = np.vstack((r*np.cos(theta), r*np.sin(theta))).T
    cls = np.hstack( ([1]*100, [2]*200, [3]*200))
    bullseye = pd.DataFrame( np.vstack((inner,middle,outer)), columns=["x1", "x2"] )
    bullseye["class"] = cls 
    return bullseye
```

```{python}
bullseye = bullseye_data()
p = sns.relplot(bullseye,
        x="x1", y="x2", 
        hue="class"
        )
p.set(aspect=1);
```

## Similarity and distance

Given an $n\times d$ feature matrix, we want to define disjoint subsets of the $\bfx_i$ such that the samples within a subset, or **cluster**, are more similar to one another than they are to members of other clusters.

The first decision we have to make is how to measure *similarity*. When a distance metric is available, we consider similarity to be inversely related to distance. For example, if we have defined a distance function between pairs of vectors as $\dist(\bfx,\bfy)$, then we could define similarity as 

$$
\simil(\bfx,\bfy) = \exp \left[ - \frac{\dist(\bfx,\bfy)^2}{2\sigma^2}  \right].
$$

Thus a distance of zero implies a similarity of 1, while the similarity tends to zero as distance increases. The scaling parameter $\sigma$ controls the rate of decrease; for instance, when the distance is $\sigma$, the similarity is $e^{-1/2}\approx 0.6$. 

There are ways to define similarity without making use of a distance, but we won't be using them.

### Distance metrics

::::{#def-cluster-metric}
A **distance metric** is a function dist on pairs of vectors that satisfies the following properties for all vectors:

1. $\dist(\bfx,\bfy)=0$ if and only if $\bfx=\bfy$,
2. $\dist(\bfx,\bfy) = \dist(\bfy,\bfx)$, and
3. $\dist(\bfx,\bfy) \le \dist(\bfx,\bfz) + \dist(\bfz,\bfy)$, known as the triangle inequality.
::::

These are considered the essential axioms of a distance metric. From them, you can also deduce that the distance function is always nonnegative. 

::: {.callout-caution}
The term *distance metric* isn't always used carefully to mean a function satisfying the three axioms, however, and some applications use a metric that does not satisfy the triangle inequality. 
:::

We already have the distance metric defined by

$$
\dist(\bfx,\bfy) = \norm{\bfx-\bfy}
$$

for any vector norm. 

Another proper distance metric is **angular distance**. Generalizing from 2D and 3D vector geometry, we define the angle $\theta$ between vectors $\bfu$ and $\bfv$ in $\real^d$ by

$$ 
\cos(\theta) = \frac{\mathbf{u}^T\mathbf{v}}{\twonorm{\mathbf{u}} \, \twonorm{\mathbf{v}}}.
$$ {#eq-clustering-angle}

Then the quantity $\theta/\pi$ is a distance metric. Because arccos is a relatively expensive computational operation, though, it's common to use **cosine similarity**, defined as $\cos(\theta)$, and the related **cosine distance** $\tfrac{1}{2}[1-\cos(\theta)]$, even though the latter does not satisfy the triangle inequality.

Categorical variables can be included in distance metrics. An ordinal variable is easily converted to equally spaced numerical values, which then may get a standard treatment. Nominal features are often compared using **Hamming distance**, which is just the total number of features that have different values in the two vectors.

### Probability distributions

::::{#def-cluster-probdist}
A **discrete probability distribution** is a vector $\bfx$ whose components are nonnegative and satisfying $\onenorm{\bfx}=1$. 
::::

Such a vector can be interpreted as frequencies or probabilities of observing different classes, for example. We already encountered one way to measure the dissimilarity of two probability distributions, the *cross-entropy*:

$$
\operatorname{CE}(\bfx,\bfy) = -\sum_{i=1}^d x_i \log(y_i). 
$$

A related measure is the **Kullbackâ€“Leibler (KL) divergence** or *relative entropy*,

$$
\operatorname{KL}(\bfx,\bfy) = \sum_{i=1}^d x_i \log\left( \frac{x_i}{y_i} \right).
$$

Whenever $0\cdot \log(0)$ is encountered in the CE or KL definitions, it equals zero, in accordance with its limiting value from calculus.

Neither cross-entropy nor KL divergence are symmetric in their arguments. But there is a related value called **information radius**, defined as

$$
\operatorname{IR}(\bfu,\bfv) = \frac{1}{2} \left[ \operatorname{KL}(\bfu,\bfz) + \operatorname{KL}(\bfv,\bfz) \right]
$$

where $\bfz=(\bfu+\bfv)/2$. Typically one uses a base-2 logarithm, in which case IR ranges between 0 and 1. The square root of IR is a distance metric.

::::{#exm-cluster-IR}
Let $\bfu=\frac{1}{4}[1,3]$ and $\bfv=\frac{1}{4}[3,1]$. Then $\bfz=(\bfu+\bfv)/2=[\tfrac{1}{2},\tfrac{1}{2}]$, and

$$
\begin{split}
    \operatorname{KL}(\bfu,\bfz)  &= \tfrac{1}{4} \cdot \log \left( \frac{1/4}{1/2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3/4}{1/2} \right) \\ &= \tfrac{1}{4} \cdot \log \left( \frac{1}{2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3}{2} \right) = -\tfrac{1}{4} + \tfrac{3}{4} (\log(3)-1),\\
    \operatorname{KL}(\bfv,\bfz)  &= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4},\\
    \operatorname{IR}(\bfu,\bfu)  &= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4} \approx 0.1887.
\end{split}
$$
::::

### Distance matrix

::::{#def-cluster-distmat}
Given the feature vectors $\bfx_1,\ldots,\bfx_n$, the pairwise distances between them are collected in the $n\times n$ **distance matrix**

$$
D_{ij} = \text{dist}(\bfx_i,\bfx_j).
$$
::::

Note that $D_{ii}=0$ and $D_{ji}=D_{ij}$. Many clustering algorithms allow supplying $\mathbf{D}$ in lieu of the feature vectors.

One can analogously define a *similarity matrix* using the Gaussian kernel. An advantage of similarity is that small values can be rounded down to zero. This has little effect on the results, but can create big gains in execution time and memory usage. 

::::{#exm-cluster-distmat}
The distance matrix of our bullseye dataset has some interesting structure:
```{python}
from sklearn.metrics import pairwise_distances
X = bullseye_data()[["x1", "x2"]]
D2 = pairwise_distances(X, metric="euclidean")   # use 2-norm metric
ax = sns.heatmap(D2)
ax.set_aspect(1);
```
Because we set up three geometrically distinct groups of points, the distances of pairs within and between groups are fairly homogeneous. The lower-right corner, for example, shows that points in the outermost ring tend to be separated by the greatest distance.

In the 1-norm, the stripes dataset is also a little interesting:
```{python}
X = stripes_data()[["x1", "x2"]]
D1 = pairwise_distances(X, metric="manhattan")   # use 1-norm metric
ax = sns.heatmap(D1)
ax.set_aspect(1);
```
Points in different stripes are always separated by at least the inter-stripe distance, while points within the same stripe have a range of possible distances.
::::

### Distance in high dimensions

High-dimensional space [does not conform to some intuitions](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) formed by our experiences in 2D and 3D. 

For example, consider the unit hyperball $\twonorm{\bfx}\le 1$ in $d$ dimensions. We'll take it as given that scaling a $d$-dimensional object by a number $r$ will scale the volume by $r^d$. Then for any $r<1$, the fraction of the unit hyperball's volume lying *outside* the smaller hyperball of fixed radius $r$ is $1-r^d$, which approaches $1$ as $d\to \infty$. That is, *if we choose points randomly within a hyperball, almost all of them will be near the outer boundary*. 

The volume of the unit hyperball also vanishes as $d\to \infty$. This is because the inequality

$$
x_1^2 + x_2^2 + \cdots + x_d^2 \le 1,
$$

where each $x_i$ is chosen randomly in $[-1,1]$, becomes ever harder to satisfy as the number of terms in the sum grows, and the relative occurrence of such points is increasingly rare.

There are other, similar mathematical results demonstrating the unexpectedness of distances in high-dimensional space. These go under the colorful name *curse of dimensionality*, and the advice given in response to them is sometimes stated flatly as, "Don't use distance metrics in high-dimensional space." 

But that advice is easy to overstate. The curse is essentially about *randomly* chosen points, and it is correct that dimensions of noisy or irrelevant features will make many learning algorithms less effective. But if features carry useful information, adding them usually makes matters better, not worse.

## Performance measures

Before we start generating clusterings, we need to decide how we will evaluate them. Recall that a clustering is simply a partitioning of the sample points into disjoint subsets. If a classification of the samples is available, then it automatically implies a clustering: divide the samples into subsets determined by class membership.

We will use some nonstandard terminology that makes the definitions a bit easier to state and read.

::::{#def-cluster-buddies}
We say that two sample points in a clustering are **buddies** if they are in the same cluster, and **strangers** otherwise. 
::::

### Rand index and ARI

If a trusted or reference clustering is available, then we can compare it to any other clustering result. This allows us to use classification datasets as proving grounds for clustering, although the problems of classification and clustering have different goals (separation versus similarity).

Let $b$ be the number of pairs that are buddies in both clusterings, and let $s$ be the number of pairs that are strangers in both clusterings. Noting that there are $\binom{n}{2}$ distinct pairs of $n$ sample points, we define the **Rand index** by

$$
\text{RI} = \frac{b+s}{\binom{n}{2}}.
$$

One way to interpret the Rand index is through binary classification: if we define a positive result on a pair of samples to mean "in the same cluster" and a negative result to mean "in different clusters", then the Rand index is the accuracy of the classifier over all pairs of samples.

::::{#exm-cluster-rand}
Suppose that samples $x_1,x_2,x_4$ are classified as blue, and $x_3,x_5$ are classified as red. Let's compute the Rand index relative to the reference classification for the clustering $A=\{x_1,x_2\}$ and $B=\{x_3,x_4,x_5\}$.

Here is a table showing which pairs of samples are buddies in both clusterings (indicated as TP), strangers in both (TN), or neither (F).

|       | $x_1$ | $x_2$ | $x_3$ | $x_4$ | $x_5$ |
|--|--|--|--|--|--|
| $x_1$ |       |  TP   |  TN   |  F    |  TN   |
| $x_2$ |       |       |  TN   |  F    |  TN   |
| $x_3$ |       |       |       |  F    |  TP   |
| $x_4$ |       |       |       |       |  F    |
| $x_5$ |       |       |       |       |       |

Hence the Rand index is 6/10 = 0.6.
::::

The Rand index has some attractive features:

* It is symmetric in the two clusterings; it doesn't matter which is considered the reference.
* There is no need to find a correspondence between the clusters in the two clusterings. In fact, the clusterings need not even have the same number of clusters. 
* The value is between 0 (complete disagreement) and 1 (complete agreement).

A weakness of the Rand index is that it can be fairly close to 1 even for a random clustering. The **adjusted Rand index** is

$$
\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\text{max}(\text{RI})-E[\text{RI}]},
$$

where the mean and max operations are taken over all possible clusterings. (These values can be worked out exactly by combinatorics.) The value can be negative. An ARI of 0 indicates no better agreement than a random clustering, and an ARI of 1 is complete agreement. 

### Silhouettes

If no reference clustering is available, then we must use an intrinsic measurement to assess quality. Suppose $\bfx_i$ is a sample point. Let $\bar{b}_i$ be the mean distance between $\bfx_i$ and its buddies, and let $\bar{r}_i$ be the mean distance between $\bfx_i$ and the members of the nearest cluster of strangers. Then the **silhouette value** of $\bfx_i$ is 

$$
s_i = \frac{\bar{r}_i-\bar{b}_i}{\max\{\bar{r}_i,\bar{b}_i\}}.
$$

This value is between $-1$ (bad) and $1$ (good) for every sample point. A **silhouette score** is derived by taking a mean of the silhouette values, either per cluster or overall depending on the usage.

::::{#exm-cluster-silhouette}
Suppose that two clusters in one dimension are defined as $A=\{-4,-1,1\}$ and $B=\{2,6\}$. Find the silhouette values of all the samples, the silhouette scores of the clusters, and the overall silhouette score.

|  $x_i$ | $\bar{b}_i$ | $\bar{r}_i$ | $s_i$  | 
|:-------------:|:---------:|:----------:|:------:|
| $-4$   | $\frac{3+5}{2}$  | $\frac{6+10}{2}$  |  $\frac{8-4}{8}=\frac{1}{2}$  | 
| $-1$   | $\frac{3+2}{2}$  | $\frac{3+7}{2}$  |  $\frac{5-2.5}{5}=\frac{1}{2}$  | 
| $1$   | $\frac{5+2}{2}$  | $\frac{1+5}{2}$  |  $\frac{3-3.5}{3.5}=-\frac{1}{7}$  | 
| $2$   | $\frac{4}{1}$  | $\frac{6+3+1}{3}$  |  $\frac{(10/3)-4}{4}=-\frac{1}{6}$  | 
| $6$   | $\frac{4}{1}$  | $\frac{10+7+5}{3}$  |  $\frac{(22/3)-4}{(22/3)}=\frac{5}{11}$  | 

The silhouette score of cluster $A$ is 

$$
\frac{1}{3}\left( \frac{1}{2} + \frac{1}{2} - \frac{1}{7} \right) \approx 0.286,
$$

and of cluster $B$ is 

$$
\frac{1}{2}\left( \frac{5}{11} - \frac{1}{6} \right) \approx 0.144. 
$$

The overall score is the mean of the five values in the last column, which is about $0.229$.
::::

The silhouette score is fairly easy to understand and use. However, it relies on distances and tends to favor convex, compact clusters.

::::{#exm-cluster-perform-blobs}
Let's use the predefined cluster assignments in our blobs dataset. We will add a column to the data frame that records the silhouette score for each point:

```{python}
from sklearn.metrics import silhouette_samples

blobs = blobs_data()
X = blobs.drop("class", axis=1)

blobs["sil"] = silhouette_samples(X, blobs["class"])
sns.relplot(blobs,
    x="x1", y="x2",
    hue="class",size="sil"
    );
```

In the plot above, the size of each dot shows its silhouette coefficient. Those points which don't belong comfortably with their cluster have negative scores and the smallest dots. We can find the average score in each cluster through a grouped mean:

```{python}
blobs.groupby("class")["sil"].mean()
```

These values are ordered as we would expect. Now let's create another clustering based on the quadrants of the plane:

```{python}
def quad(x,y):
    if x > 0:
        if y > 0: return 1
        else: return 4
    else:
        if y > 0: return 2
        else: return 3

blobs["quadrant"] = [quad(x,y) for (x,y) in zip(blobs.x1, blobs.x2)]
blobs["sil"] = silhouette_samples(X, blobs["quadrant"])
sns.relplot(blobs, 
    x="x1", y="x2",
    hue="quadrant",size="sil"
    );
```

```{python}
blobs.groupby("quadrant")["sil"].mean()
```

Even though the original clustering had three classes, and there are four quadrants, we can still compare them by adjusted Rand index:

```{python}
from sklearn.metrics import adjusted_rand_score

adjusted_rand_score(blobs["class"], blobs["quadrant"])
```

Not surprisingly, they are seen as fairly similar.
::::

::::{#exm-cluster-perform-digits}
`sklearn` has a well-known dataset that contains labeled handwritten digits. Let's extract the examples for just the numerals 4, 5, and 6:

```{python}
digits = datasets.load_digits(as_frame=True)["frame"]
keep = digits["target"].isin([4,5,6])
digits = digits[keep]
X = digits.drop("target", axis=1)
y = digits.target
y.value_counts()
```

We can visualize the raw data. Here are some of the 6s:

```{python}
#| code-fold: true
import matplotlib.pyplot as plt
import numpy as np

def plot_digits(X):
    fig, axes = plt.subplots(4,4)
    for i in range(4):
        for j in range(4):
            row = j + 4*i
            A = np.reshape(np.array(X.iloc[row,:]),(8,8))
            sns.heatmap(A,ax=axes[i,j],square=True,cmap="gray",cbar=False)
            axes[i,j].axis(False)
    return None

plot_digits(X[y==6])
```

A clustering method won't be able to learn from the ground truth labels. In order to set expectations, we should see how well the originally labels cluster the samples. Here are the mean silhouette scores for the clusters.

```{python}
digits["sil"] = silhouette_samples(X,y)
digits.groupby("target")["sil"].mean()
```

As usual, means can tell us only so much. A look at the distributions of the values reveals more details:

```{python}
sns.catplot(digits,
    x="target", y="sil",
    kind="violin"
    );
```

The values are mostly positive, which indicates nearly all of the samples for a digit are at least somewhat closer to each other than to the other samples. The 6s are the most distinct. The existence of values close to and below zero suggest that a clustering algorithm might reconstruct the classification to some extent, but the ground truth may represent something more than geometric distances in feature space.
::::

::: {.callout-important}
The universe doesn't owe you a clustering. Not all phenomena are amenable to clustering in whatever features you happen to choose.
:::

While classification just requires us to separate different classes of examples, clustering is more specific and more demanding: examples in a cluster need to be more like each other, or the "average" cluster member, than they are like members of other clusters. We should expect that edge cases, even within the training data, will look ambiguous.

## k-means

The **$k$-means algorithm** is one of the best-known and most widely used clustering methods, although it has some serious limitations and drawbacks. 

Given a sample matrix $\bfX$ with $n$ rows $\bfx_i$, the algorithm divides the sample points into disjoint sets $C_1,\ldots,C_k$, where $k$ is a preselected hyperparameter. Cluster $j$ has a **centroid** $\bfmu_j$, which is the mean of the points in $C_j$. Define the **inertia** of $C_j$ as 

$$
I_j = \sum_{\bfx\in C_j} \norm{ \bfx - \bfmu_j }_2^2.
$$

The goal of the algorithm is to choose the clusters in order to minimize the total inertia,

$$
I = \sum_{j=1}^k I_j.
$$

<!-- For any cluster, its centroid is the point that minimizes the inertia of the cluster. Suppose that $C_j$ is split into two parts $A$ and $B$ that have centroids $\bfmu_A$ and $\bfmu_B$. Those centroids minimize the inertias of the subclusters. Hence, 

$$
\sum_{\bfx\in A} \norm{ \bfx - \bfmu_A }^2 + \sum_{\bfx\in B} \norm{ \bfx - \bfmu_B }^2 
\le  \sum_{\bfx\in A} \norm{ \bfx - \bfmu_j }^2 + \sum_{\bfx\in B} \norm{ \bfx - \bfmu_j}^2  = I_j. 
$$

We conclude that splitting a cluster will make the total inertia decrease. In fact, if each sample point is put into its own cluster, the inertia is 0.  -->

::::{#exm-cluster-inertia}
Let $k=2$. Given the values $-3,-2,-1,2,5,7$, we might cluster $\{-3,-2,-1\}$ and $\{2,5,7\}$. The total inertia is then

$$
\left[  (-3+2)^2 + (-2+2)^2 + (-1+2)^2   \right]  + \left[  \bigl(2-\tfrac{14}{3}\bigr)^2 + \bigl(5-\tfrac{14}{3}\bigr)^2 + \bigl(7-\tfrac{14}{3}\bigr)^2   \right] = 2 + \frac{124}{9} = 15.78.
$$

If we instead cluster as $\{-3,-2,-1,2\}$ and $\{5,7\}$, then the total inertia is 

$$
\left[  (-3+1)^2 + (-2+1)^2 + (-1+1)^2  + (2+1)^2 \right]  + \left[   (5-6)^2 + (7-6)^2   \right] = 14 + 2 = 16.
$$
::::

Finding the minimum inertia among all possible $k$-clusterings is an infeasible problem to solve exactly at any practical size. Instead, the approach is to iteratively improve from a starting clustering.

### Lloyd's algorithm

The standard method is known as **Lloyd's algorithm**. Starting with values for the $k$ centroids, there is an iteration consisting of two steps:

* **Assignment** Each sample point is assigned to the cluster whose centroid is the nearest. (Ties are broken randomly.)
* **Update** Recalculate the centroids based on the cluster assignments:

$$
\bfmu_j^+ = \frac{1}{|C_j|} \sum_{\bfx\in C_j} \bfx.
$$

The algorithm stops when the assignment step does not change any of the clusters. In practice, this almost always happens quickly. Here is a demonstration:

{{< video _media/kmeans_demo.mp4 width="480" >}}

While Lloyd's algorithm will find a local minimum of total inertia, in the sense that small changes cannot decrease it, there is no guarantee of converging to the global minimum.

### Practical issues

* **Initialization**. The performance of $k$-means depends a great deal on the initial set of centroids. Traditionally, the centroids were chosen as random members of the sample set, but better/more reliable heuristics, such as *$k$-means++*, have since become more dominant. 
* **Multiple runs**. All the initialization methods include an element of randomness, and since the Lloyd algorithm usually converges quickly, it is often run with multiple instances of the initialization, and the run with the lowest inertia is kept.
* **Selection of $k$**. The algorithm treats $k$ as a hyperparameter. Occam's Razor dictates preferring smaller values to large ones. There are many suggestions on how to find the choice that gives the most "bang for the buck."
* **Distance metric**. The Lloyd algorithm often fails to converge for norms other than the 2-norm, and must be modified if another norm is preferred.
* **Shape effects**. Because of the dependence on the norm, the inertia criterion disfavors long, skinny clusters and clusters of unequal dispersion. Basically, it wants to find spherical blobs (as defined by the metric) of roughly equal size.

::::{#exm-cluster-kmeans-blobs}
Let's generate some test blobs:

We start $k$-means with $k=2$ clusters, not presupposing prior knowledge of how the samples were created.

```{python}
from sklearn.cluster import KMeans

X = blobs_data()[["x1", "x2"]]
km2 = KMeans(n_clusters=2, n_init="auto")
km2.fit(X)
```

The fitted clustering object can tell how many iterations were required, and what the final inertia and cluster centroids are:

```{python}
print("k=2 took",km2.n_iter_,"iterations")
print(f"final inertia: {km2.inertia_:.5g}")
print("cluster centroids:")
print(km2.cluster_centers_)
```

There is a `predict` method that can make cluster assignments for arbitrary points in feature space. In k-means, this just tells you which centroid is closest, i.e., the cluster membership:

```{python}
km2.predict([ [-2,-1], [1,2] ])
```

For the training samples we don't need to call `predict`. Every fitted clustering object has a `labels_` property that lists the of cluster index values. We can use those labels to compute silhouette scores:

```{python}
def report(clustering):
    blobs["cluster"] = clustering.labels_
    blobs["sil"] = silhouette_samples(X, blobs["cluster"])
    print(f"inertia: {clustering.inertia_:.5g}")
    print(f"overall silhouette score: {blobs['sil'].mean():.5g}")
    sns.catplot(blobs,
        x="cluster", y="sil",
        kind="violin", height=3.5
        );
    sns.relplot(blobs,
        x="x1", y="x2",
        hue="cluster", size=blobs["sil"], height=3.5
        );
    return blobs 

report(km2)
```

It's clear in both plots that cluster 0 is more tightly packed than cluster 1. Let's repeat the computation for $k=3$ clusters:

```{python}
km3 = KMeans(n_clusters=3, n_init="auto")
km3.fit(X)
report(km3)
```

This result shows a modest reduction in silhouette scores for original good cluster, but improvement for the problematic one.

Moving on to $k=4$ clusters shows clear degradation of the silhouette scores:

```{python}
km4 = KMeans(n_clusters=4, n_init="auto")
km4.fit(X)
report(km4)
```

Based on silhouette scores, then, we would probably stop at $k=3$ clusters. 
::::

::::{#exm-cluster-kmeans-stripes}
K-means is expecting to find roughly spherical clusters. When the data do not conform to that model, it tends to perform poorly:

```{python}
stripes = stripes_data()
X = stripes[["x1", "x2"]]
results = pd.DataFrame()
for k in [2,3,4]:
    km = KMeans(n_clusters=k, n_init="auto")
    km.fit(X)
    stripes["cluster"] = km.labels_
    stripes["k"] = k
    results = pd.concat( (results, stripes) )
    
sns.relplot(results,
    x="x1", y="x2",
    hue="cluster", col="k", height=3.5
    );
```

It's not a bad idea to standardize the data. But that's no panacea:
```{python}
results = pd.DataFrame()
for k in [2,3,4]:
    km = make_pipeline(StandardScaler(), KMeans(n_clusters=k, n_init="auto"))
    km.fit(X)
    stripes["cluster"] = km[1].labels_
    stripes["k"] = k
    results = pd.concat( (results, stripes) )
    
sns.relplot(results,
    x="x1", y="x2",
    hue="cluster", col="k", height=3.5
    );
```
Clustering is hard!
::::

::::{#exm-cluster-kmeans-digits}
We return to the handwriting recognition dataset. Again we keep only the samples labeled 4, 5, or 6:

```{python}
digits = datasets.load_digits(as_frame=True)["frame"]
keep = digits["target"].isin([4,5,6])
digits = digits[keep]

X = digits.drop("target",axis="columns")
y = digits["target"]
```

We fit 3 clusters to the feature matrix:

```{python}
km = KMeans(n_clusters=3)
km.fit(X)
digits["kmeans3"] = km.labels_
digits[["target", "kmeans3"]].head(9)
```

The adjusted Rand index suggests that we have reproduced the classification very well:

```{python}
ARI = adjusted_rand_score(y, digits["kmeans3"])
print(f"ARI: {ARI:.4f}")
```

However, that conclusion benefits from our prior knowledge. What if we did not know how many clusters to look for? Let's look over a range of $k$ values, recording the final total inertia and the mean silhouette score for each

```{python}
from sklearn.metrics import silhouette_score
results = []
for k in range(2,8):
    km = KMeans(n_clusters=k, random_state=0)
    km.fit(X)

    sil = silhouette_score(X, km.labels_)
    results.append( [k, sil] )

pd.DataFrame(results, columns=["k", "mean silhouette"])
```

The silhouette score is maximized at $k=3$, which could be considered a reason to choose 3 clusters. While the score for 4 clusters is fairly close, we should prefer the less complex model.
::::

## Hierarchical clustering

The idea behind hierarchical clustering is to organize all the sample points into a tree structure called a **dendrogram**. At the root of the tree is the entire sample set, while each leaf of the tree is a single sample vector. Groups of similar samples are connected as nearby relatives in the tree, with less-similar groups located as more distant relatives.

Dendrograms can be found by starting with the root and recursively splitting, or by starting at the leaves and recursively merging. We will describe the latter approach, known as **agglomerative clustering**.

The algorithm begins with $n$ singleton clusters, i.e., $C_i=\{\bfx_i\}$. Then, the similarity or distance between each pair of clusters is determined. The pair with the minimum distance is merged, and the process repeats. 

Common ways to define the distance between two clusters $C_i$ and $C_j$ are:

* single linkage
: (also called *minimum linkage*)
$$
\displaystyle \min_{\bfx\in C_i,\,\bfy\in C_j} \{ \norm{\bfx-\bfy } \}
$$ {#eq-cluster-linkage-single}

* complete linkage
: (also called *maximum linkage*)
$$
\displaystyle \max_{\bfx\in C_i,\,\bfy\in C_j} \{ \norm{\bfx-\bfy} \}
$$ {#eq-cluster-linkage-complete}

* average linkage
: 
$$
\displaystyle \frac{1}{|C_i|\,|C_j|} \sum_{\bfx\in C_i,\,\bfy\in C_j} \norm{ \bfx-\bfy }
$$ {#eq-cluster-linkage-average}

* Ward linkage
: The increase in inertia resulting from merging $C_i$ and $C_j$, equal to

$$
\frac{ |C_i|\,|C_j| }{|C_i| + |C_j|} \norm{\bfmu_i - \bfmu_j}_2^2,
$$ {#eq-cluster-linkage-ward}

where $\bfmu_i$ and $\bfmu_j$ are the centroids of $C_i$ and $C_j$.

Agglomerative clustering with Ward linkage amounts to trying to minimize the increase of inertia with each merger. In that sense, it has the same objective as $k$-means, but it is usually not as successful at minimizing inertia.

Single linkage only pays attention to the gaps between clusters, not the size or spread of them. Complete linkage, on the other hand, wants to keep clusters packed tightly together. Average linkage is a compromise between these extremes. All three of these options can work with a distance matrix in lieu of the original feature matrix.

::::{#exm-cluster-linkage-tiny}
Given clusters $C_1=\{-3,-2,-1\}$ and $C_2=\{3,4,5\}$, we find the different linkages between them:

**Ward.** The centroids of the clusters are $-2$ and $4$. So the linkage is 

$$
\frac{3\cdot 3}{3+3} \, 6^2 = 54.
$$

**Single.** The pairwise distances between members of $C_1$ and $C_2$ form a $3\times 3$ matrix:

|   | -3 | -2 | -1 |
|--|--|--|--|
3 | 6 | 5 | 4 |
4 | 7 | 6 | 5 |
5 | 8 | 7 | 6 |

The single linkage is therefore 4.

**Complete.** The maximum of the matrix above is 8.

**Average.** The average value of the matrix entries is $54/9$, which is 6.
::::

::::{#exm-cluster-hier-toy}
Let's use 5 sample points in the plane, and agglomerate them by single linkage. The `pairwise_distances` function converts sample points into a distance matrix:

```{python}
X = np.array( [[-2,-1] ,[2,-2], [1,0.5], [0,2], [-1,1]] )
D = pairwise_distances(X, metric="euclidean")
D
```

The minimum value in the upper triangle of the distance matrix is in row 3, column 4 (starting index at 0). So our first merge results in the cluster $C_1=\{\bfx_3,\bfx_4\}$. The next-smallest entry in the upper triangle is at position $(2,3)$, so we want to merge those samples together next, resulting in

$$
C_1=\{\bfx_2,\bfx_3,\bfx_4\},\, C_2 = \{\bfx_0\},\, C_3=\{\bfx_1\}.
$$

The next-smallest element in the matrix is at $(2,4)$, but those points are already merged, so we move on to position $(0,4)$. Now we have 

$$
C_1=\{\bfx_0,\bfx_2,\bfx_3,\bfx_4\},\, C_2 = \{\bfx_1\}. 
$$

The final merge is to combine these. 

The entire dendrogram can be visualized with seaborn:

```{python}
sns.clustermap(X, 
    col_cluster=False,
    dendrogram_ratio=(.75,.15)
    );
```

The horizontal position in the dendrogram above indicates the linkage strength. Note on the right that the ordering of the samples has been changed (so that the lines won't cross each other). The two colored columns show a heatmap of the two features of the sample points. Working from right to left, we see the merger of samples 3 and 4, which are then merged with sample 2, etc. 

In effect, we get an entire family of clusterings by stopping at any linkage value we want. If we chose to stop at value 2.5, for instance, we would have two clusters of size 4 and 1. Or, if we predetermine that we want $k$ clusters, we can stop after $n-k$ merge steps.
::::

::::{#exm-cluster-linkage}
We define a function that allows us to run all three linkages for a dataset:
```{python}
from sklearn.cluster import AgglomerativeClustering

def run_experiment(data):
    results = pd.DataFrame()
    for linkage in ["single", "complete", "ward"]:
        agg = AgglomerativeClustering(n_clusters=3, linkage=linkage)
        agg.fit( data[["x1", "x2"]] )
        data["cluster"] = agg.labels_
        data["linkage"] = linkage
        results = pd.concat( (results, data) )
    return results
```

We first try the blobs seen previously:

```{python}
#| code-fold: true
from sklearn.datasets import make_blobs
X, y = make_blobs(
    n_samples=[60, 50, 40],
    centers=[ [-2,3], [3.5,1.5], [1,-3] ],
    cluster_std=[0.5, 0.9, 1.2],
    random_state=19716
    )
blobs = pd.DataFrame( {"x1": X[:,0], "x2": X[:,1], "class": y} )
blobs.head()
```

```{python}
#| column: body-outset
results = run_experiment(blobs)
sns.relplot(results,
        x="x1", y="x2", 
        hue="cluster", col="linkage", height=4
        )
```

As you can see, the simple linkage was confused by the two blobs that nearly run together. 

Next, we try data points lying in three distinct stripes:

```{python}
#| column: body-outset
stripes = stripes_data()
results = run_experiment(stripes)
sns.relplot(results,
        x="x1", y="x2", 
        hue="cluster", col="linkage", height=4
        )
```

Both the complete and Ward linkages are committed to finding compact, roughly spherical clusters. They group together points across stripes rather than clusters extending lengthwise. The single linkage has more flexibility.

Finally, we try the most demanding test, points that are arranged in rings:

```{python}
#| column: body-outset
bullseye = bullseye_data()
results = run_experiment(bullseye)
p = sns.relplot(results,
        x="x1", y="x2", 
        hue="cluster", col="linkage", height=4
        )
p.set(aspect=1);
```

The single linkage is the only one with enough geometric flexibility to cluster the rings properly. However, it's a delicate situation, and it can be sensitive to individual samples. Here, we add just one point to the bullseye picture and get a big change:

```{python}
#| column: body-outset
bullseye = pd.concat( ( bullseye, pd.DataFrame({"x1": [0], "x2": [2.25]}) ) )
results = run_experiment(bullseye)
p = sns.relplot(results,
        x="x1", y="x2", 
        hue="cluster", col="linkage", height=4
        )
p.set(aspect=1);
```
::::

### Case study: Penguins

Let's try agglomerative clustering to discover the species of the penguins. First, let's recall how many of each species we have.

```{python}
penguins = sns.load_dataset("penguins").dropna()
features = ["bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"]
X = penguins[features]
penguins["species"].value_counts()
```

Our first attempt is single linkage. Because 2-norm distances are involved, we will use standardization in a pipeline with the clustering method. After fitting, the `labels_` property of the cluster object is a vector of cluster assignments.

```{python}
from sklearn.cluster import AgglomerativeClustering

single = AgglomerativeClustering(n_clusters=3, linkage="single")
pipe = make_pipeline(StandardScaler(),single)
pipe.fit(X)
penguins["single"] = single.labels_       # cluster assignments
penguins.loc[::24,["species", "single"]]   # print out some rows
```

It seems that Gentoo is associated with cluster number 2, but the situation with the other species is less clear. Here are the value counts:

```{python}
print("single linkage results:")
penguins["single"].value_counts()
```

As we saw with the toy datasets in @exm-cluster-linkage, the single linkage is susceptible to declaring one isolated point to be a cluster, while grouping together other points we would like to separate. Here is the ARI for this clustering, compared to the true classification:

```{python}
from sklearn.metrics import adjusted_rand_score
ARI = adjusted_rand_score(penguins["species"],penguins["single"])
print(f"single linkage ARI: {ARI:.4f}")
```

Now let's try it with Ward linkage (the default):

```{python}
ward = AgglomerativeClustering(n_clusters=2, linkage="ward")
pipe = make_pipeline(StandardScaler(), ward)
pipe.fit(X)
penguins["ward"] = ward.labels_

print("Ward linkage results:")
print(penguins["ward"].value_counts())
```

This result looks more promising. The ARI confirms that hunch:

```{python}
ARI = adjusted_rand_score(penguins["species"], penguins["ward"])
print(f"Ward linkage ARI: {ARI:.4f}")
```

If we guess at the likely correspondence between the cluster numbers and the different species, then we can find the confusion matrix:

```{python}
from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay
y = penguins["species"]
# Convert cluster numbers into labels:
y_hat = penguins["ward"].replace({1:"Adelie",0:"Gentoo",2:"Chinstrap"}) 

ConfusionMatrixDisplay(confusion_matrix(y,y_hat),display_labels=y.unique()).plot();
```

## Exercises {.unnumbered}

::::{#exr-cluster-positive}
Using only the three axioms of a distance metric, prove that $\dist(\bfx,\bfy) \ge 0$ for all vectors $\bfx$ and $\bfy$. (Hint: apply the triangle inequality to go from $\bfx$ to $\bfy$ and back again.)
::::

::::{#exr-cluster-angular}
Prove that the angular distance between any nonzero vector and itself is zero.
::::

::::{#exr-cluster-cosine}
Find a counterexample showing that cosine distance does not satisfy the triangle inequality. (Hint: it's enough to consider some simple vectors in two dimensions.)
::::

::::{#exr-cluster-inertia}
Let $c$ be a positive number, and consider the 12 sample points $\{(\pm c,\pm j): j=1,2,3\}$. One way to cluster the sample points, which we designate as clustering $\alpha$, is to split according to the sign of $x_1$. Another way, which we designate as clustering $\beta$, is to split according to the sign of $x_2$. Compute the inertia of both clusterings. For which values of $c$, if any, does clustering $\alpha$ have less inertia than clustering $\beta$?
::::

::::{#exr-cluster-linkage}
Here is a distance matrix for points $\bfx_1,\ldots,\bfx_5$. 

$$
\left[
\begin{array}{ccccc}
0 & 2 & 4 & 5 & 6 \\
2 & 0 & 2 & 3 & 4 \\
4 & 2 & 0 & 1 & 2 \\
5 & 3 & 1 & 0 & 1 \\
6 & 4 & 2 & 1 & 0 \\
\end{array}
\right]
$$

Compute the average linkage between the clusters with index sets $C_1=\{1,3\}$ and $C_2=\{2,4,5\}$. 
::::

::::{#exr-cluster-agglom}
Perform by hand an agglomerative clustering for the values $2,4,5,8,12$ using single linkage. This means finding the four merge steps needed to convert five singleton clusters into one global cluster.
::::
