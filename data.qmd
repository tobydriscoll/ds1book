# Representation of data
{{< include _macros.qmd >}}

First we have to discuss how to represent data, both abstractly and in Python.

## Quantitative data

:::{#def-data-quantitative}
A **quantitative** value is one that is numerical and supports meaningful comparison and arithmetic operations.
:::

 Quantitative data is further divided into **continuous** and **discrete** types. The difference is the same as between real numbers and integers.

::: {#exm-data-quantitative}

Some continuous quantitative data sources:

* Temperature at noon at a given airport
* Your height
* Voltage across the terminals of a battery

Examples of discrete quantitative data:

* The number of shoes you own
* Number of people at a restaurant table
* Score on an exam

In each case, it makes sense, for example, to order different values and compute averages of them. However, averages of discrete quantities are continuous.
:::

:::{.callout-note}
Sometimes there may be room for interpretation or context. For example, the retail price of a gallon of milk might be regarded as discrete data, since it technically represents a whole number of pennies. But in finance, transactions are regularly computed to much higher precision, so it might make more sense to interpret prices as continuous values.
:::

::: {#exm-data-zip}
Not all numerical values represent truly quantitative data. ZIP codes (postal codes) in the U.S. are 5-digit numbers, and while there is some logic to how they were assigned, there is no clearly meaningful interpretation of averaging them, for instance.
:::

Mathematically, the real and integer number sets are infinite, but that is not possible in a computer. Integers are represented exactly within some range that is determined by how many binary bits are dedicated. The computational analog of real numbers are **floating-point numbers**, or more simply, **floats**. These are bounded in range as well as discretized. The details are complicated, but essentially, the floating-point numbers have about 16 significant digits by default, which is virtually always far more precision than real data offers.

```{python}
# This is an integer (int type)
print(3, "is a", type(3))

# This is a real number (float type)
print(3.0, "is a", type(3.0))

# Convert int to float (generally no change to numerical value)
print( "float(3) creates",float(3) )

# Truncate float to int
print( "int(3.14) creates", int(3.14) )
```

### Special values

There are two additional quasi-numerical `float` values to be aware of as well.

::: {.callout-important}
For numerical work in Python, the NumPy package indispensible. We will use it often, and it is also loaded and used by most other scientifically oriented packages.
:::

The value `inf` stands for infinity. It's greater than every finite number. Some arithmetic with infinity is well-defined:

```{python}
import numpy as np
print( "np.inf + 5 is", np.inf + 5 )
print( "np.inf + np.inf is", np.inf + np.inf )
print( "5 - np.inf is", 5 - np.inf )
```

However, in calculus you learned that some expressions with infinity are considered to be undefined without additional information to apply (e.g., L'HÃ´pital's Rule):

```{python}
print( "np.inf / np.inf is", np.inf / np.inf )
```

The result `nan` stands for *Not a Number*. It is the result of indeterminate arithmetic operations, like $\infty/\infty$ and $\infty - \infty$. It is also used sometimes as a placeholder for missing data, i.e. to mean, "unknown value".

:::{.callout-warning}
By definition, every operation that involves a `NaN` value results in a `NaN`.
:::

One notorious consequence of this behavior is that `nan==nan` is `nan`, not `True`!

### Dates and times

Handling times and dates can be tricky. Aside from headaches such as time zones and leap years, there are many different ways people and machine represent dates, and with varying amounts of precision. Python has its own inbuilt system for handling dates and times, but we will show the facilities provided by NumPy instead.

There are two basic types:

::::{#def-data-time}
A **datetime** is a representation of an instant in time. A **time delta** is a representation of a duration; i.e., a difference between two datetimes.
::::

::: {.callout-important}
Native Python uses a `datetime` type, while NumPy uses `datetime64`.
:::

```{python}
np.datetime64("2020-01-17")    # YYYY-MM-DD 
```

```{python}
np.datetime64("1969-07-20T20:17")    # YYYY-MM-DDThh:mm
```

```{python}
# Current date and time, down to the second
np.datetime64("now")
```

A time delta in NumPy indicates its units (granularity).

```{python}
np.datetime64("1969-07-20T20:17") - np.datetime64("today")
```

### Random numbers

Generating truly random numbers on a computer is not simple. Mostly we rely on *pseudorandom* numbers, which are generated by deterministic functions called **random number generators** (RNGs) that have extremely long periods. One nice consequence is repeatability. By specifying the starting state of the RNG, you can get exactly the same pseudorandom sequence every time.

We will rely on pseudorandom numbers in two ways. First, many algorithms in data science have at least one random aspect (dividing data into subsets, for example). The library routines we will be using allow you to specify the random state and get repeatable results. Occasionally, though, we might want to generate random values for our own use.

```{python}
from numpy.random import default_rng
rng = default_rng(19716)    # giving an initial state
```

The `uniform` generator method produces numbers distributed uniformly (i.e., every value is equally likely) between two limits you specify.

```{python}
for _ in range(5):
    print( rng.uniform( -1, 1 ) )
```

Another common type of random value is generated by `normal`, which produces real values distributed according to the "bell curve" (normal or Gaussian distribution).

```{python}
for _ in range(5):
    print( rng.normal() )
```

In the long run, the average value of these numbers will be zero.

```{python}
s = 0
for _ in range(100000):
    s += rng.normal()

s/100000
```

## Arrays

Most interesting phenomena are characterized and influenced by more than one factor. Collections of values therefore play a huge role in data science. The workhorse type for collections in base Python is the list. However, we're going to need some more powerful metaphors and tools as well. 

### Vectors

::::{#def-data-vector}
A **vector** is a collection of values called **elements**, all of the same type, indexed by consecutive integers.
::::

::: {.callout-important}
In math, vector indexes usually begin with 1. In Python, they begin with 0. 
:::

A vector with $n$ elements is often referred to as an $n$-vector, and we say that $n$ is the **length** of the vector. In math we often use $\real^n$ to denote the set of all $n$-vectors with real-valued elements.

The usual way to work with arrays in Python is through NumPy.

```{python}
import numpy as np

x = np.array( [1,2,3,4,5] )
x
```

A vector has a data type for its elements:

```{python}
x.dtype
```

Any float values in the vector cause the data type of the entire vector to be `float`:

```{python}
y = np.array( [1.0,2,3,4,5] )
y.dtype
```

Use `len` to determine the length of a vector:

```{python}
len(x)
```

You can create a special type of vector called a *range* that has equally spaced elements:

```{python}
np.arange(0,5)
```

```{python}
np.arange(1,3,0.5)
```

The syntax here is `(start,stop,step)`. Note something critical and counterintuitive from the above results:

::: {.callout-caution}
In base Python and in NumPy, the last element of a range is omitted. This is guaranteed to cause confusion if you are used to just about any other computer language.
:::

#### Access and slicing

Use square brackets to refer to an element of a vector:

```{python}
x[0]
```

```{python}
x[4]
```

Negative values for the index are counted from the end. The last element of a vector always has index `-1`, and more-negative values move backward through the elements:

```{python}
x[-1]
```

```{python}
x[-3]
```

```{python}
x[-len(x)]
```

Element references can also be on the left side of an assigment:

```{python}
x[2] = -3
x
```

Note, however, that once the data type of a vector is set, it can't be changed:

```{python}
x[0] = 1.234
x    # float was truncated to int, without warning!
```

You can also use a list in square brackets to access multiple elements at once:

```{python}
x[ [0,2,4] ]
```

Accessing elements via a range is known as **slicing**:

```{python}
x[0:3]
```

```{python}
x[-3:-1]
```

As with ranges, the syntax of a slice is `start:stop:step`, and the last element of the range is *not* included. This causes headaches and bugs, though it does imply that the range `i:j` has $j-i$ elements, not $j-i+1$. 

When the `start` of the range is omitted, it means "from the beginning", and when `stop` is omitted, it means "through to the end." Hence, `[:k]` means "first $k$ elements" and `[-k:]` means "last $k$ elements":

```{python}
x[:3]
```

```{python}
x[-3:]
```

And we also have this idiom:

```{python}
x[::-1]   # reverse the vector
```

::: {.callout-warning}
NumPy will happily allow you to reference invalid indexes. It will just return as much as is available without warning or error.
:::

```{python}
x[:10]
```

### Multiple dimensions

A vector is an important special case of a more general construct.

::::{#def-data-array}
An **array** is a collection of values called **elements**, all of the same type, indexed by one or more sets of consecutive integers. The number of indexes needed to specify a value is the **dimension** of the array.
::::

::: {.callout-note}
A dimension is called an `axis` in NumPy and related packages.
:::

::: {.callout-note}
The term **matrix** is often used simply to mean a 2D array. Technically, though, a matrix should have only numerical values, and matrices obey certain properties that make them important mathematical objects. These properties and their consequences are studied in linear algebra.
:::

#### Construction

One way to construct an array is by a list comprehension:

```{python}
A = np.array([ [j-i for j in range(6)] for i in range(4) ])
A
```

The `shape` of an array is what we would often call the *size*:

```{python}
A.shape
```

There is no difference between a vector and a 1D array:

```{python}
x.shape
```

There is also no difference between a 2D array and a vector of vectors that give the rows of the array.

```{python}
R = np.array( [ [1,2,3], [4,5,6] ])
R
```

Here are some other common ways to construct arrays.

```{python}
np.ones(5)
```

```{python}
np.zeros( (3,6) )
```

```{python}
# See earlier section for definition of rng
rng.normal( size=(3,4) )
```

```{python}
np.repeat(np.pi, 3)
```

You can also stack arrays vertically or horizontally to create new arrays.

```{python}
np.hstack( ( np.ones((2,2)), np.zeros((2,3)) ) )
```

```{python}
np.vstack( (range(5), range(5,0,-1)) )
```

#### Indexing and slicing

We could therefore use successive brackets to refer to an element:

```{python}
R[1][2]    # second row, third column
```

But it's more convenient to use a single bracket set with indexes separated by commas:

```{python}
R[1, 2]    # second row, third column
```

You can slice in each dimension individually.

```{python}
R[:1, -2:]    # first row, last two columns
```

The result above is another 2D array. Note how this result is subtly different:

```{python}
R[0, -2:]
```

Because we accessed an individual row, not a slice, the result is one dimension lower---a vector. Finally, a `:` in one slice position means to keep everything in that dimension.

```{python}
A[:, :2]    # all rows, first 2 columns
```

#### Reductions

A common task is to *reduce* an array along one dimension, called an *axis* in numpy, resulting in an array of one less dimension. It's easiest to explain by some examples.

```{python}
np.sum(A, axis=0)    # sum along the rows
```

```{python}
np.sum(A,axis=1)    # sum along the columns
```

If you don't give an axis, the reduction occurs over all directions at once, resulting in a single number.

```{python}
np.sum(A)
```

You can also do reductions with maximum, minimum, mean, etc.

<!-- #### Broadcasting

You can add together arrays of the same shape, element by element, just as you would expect:

```{python}
x = np.array( [1,2,3,4,5] )
x + x
```

Generally, you cannot do this for arrays of different shapes:

```{python}
#| error: true
y = np.array ( [10,20] )
x+y
```


There is a major exception, though. When one of the operands can be repeated along an axis to become the same shape as the other, then that *broadcasting* is done automatically. This makes some common operations easy to express, such as adding a value to every element of an array:

```{python}
x+3
```

For a less trivial example, suppose that within each column of an array, we want to subtract off the average value in that column. This becomes easy to do via broadcasting:

```{python}
A - np.mean(A, axis=0)
```
 -->

## Qualitative data

A qualitative value is one that is not quantitative. However, in order to work with such data, we usually have to encode it in some numerical form,

### Categorical

::::{#def-data-categorical}

**Categorical** data has values drawn from a finite set $S$ of categories. If the members of $S$ support meaningful ordering comparisons, then the data is **ordinal**; otherwise, it is **nominal**.
::::

:::{#exm-data-categorical}
Examples of ordinal categorical data:

* Seat classes on a commercial airplane (e.g., economy, business, first)
* Letters of the alphabet

Examples of nominal categorical data:

* Yes/No responses
* Marital status
* Make of a car

There are nuanced cases. For instance, letter grades are themselves ordinal categorical data. However, schools convert them to discrete quantitative data and then compute a continuous quantitative GPA.
:::

One way to quantify ordinal categorical data is to assign integer values to the categories in a manner that preserves ordering. This approach can succeed, but it can be questionable when it comes to operations such as averaging or computing a distance between values. 

Another means of quantifying categorical data is called **dummy variables** in classical statistics and **one-hot encoding** in much of machine learning. Suppose a variable $x$ has values in a category set that has $m$ members we label $c_1,\ldots,c_m$. Then we can replace $x$ with introduce $m-1$ new variables $x_1,\ldots,x_{m-1}$, where
$$
x_i = \begin{cases} 1, & x=c_i, \\ 0, & x\neq c_i. \end{cases}
$$
At most one of the $x_i$ can be 1. If all of the $x_i$ are 0, then we know that $x=c_m$. 

::::{#exm-data-dummy}
Suppose that the *stooge* variable can take the values `Moe`, `Larry`, `Curly`, or `Shemp`. Then the vector

```python
[Curly, Moe, Curly, Shemp]
```

would be replaced by the array

```python
[ [0,0,1], [1,0,0], [0,0,1], [0,0,0] ]
```
::::

::: {.callout-note}
Sometimes an $m$-fold categorical variable is replaced by $m$ indicator (dummy) variables, rather than just $m-1$. 
:::

### Text 

Text is a ubiquitous data source. One way to quantify text is to use a dictionary of interesting keywords $w_1,\ldots,w_n$. Given a collection of documents $d_1,\ldots,d_m$, we can define an $m\times n$ **document--term matrix** $T$ by letting $T_{ij}$ be the number of times term $j$ appears in document $i$. 

### Images

The most straightforward way to represent an image is as a 3D array of values representing intensities representing of red, green and blue in each pixel. Sometimes it might be preferable to represent the image by a vector of statistics about these values, or by presence or absence of detected objects, etc.

## Data frames

The most popular Python package for manipulating and analyzing data is [pandas](https:pandas.pydata.org). We will use the paradigm it presents, which is fairly well understood throughout data science.

::::{#def-data-frame}
A **series** is a vector that is indexed by a finite ordered set. A **data frame** is a collection of series that all share the same index set. 
::::

We can conceptualize a series as a vector plus an index list, and a data frame as a 2D array with index sets for the rows and the columns. In that sense, they are simply syntactic sugar. However, since people are much better at remembering the meaning of words than arbitrarily assigned integers, data frames serve to prevent errors and misunderstandings.

::::{#exm-data-frames}
Some data that can be viewed as series:

* The length, width, and height of a box can be expressed as a 3-vector of positive real numbers. If we index the vector by the names of the measurements, it becomes a series.
* The number of steps taken by an individual over the course of a week can be expressed as a 7-vector of nonnegative integers. We could index it by the integers 1--7, or in a series by the names of the days of the week.
* The bid prices of a stock at the end of each trading day can be represented as a *time series*, in which the index is drawn from timestamps. 
* The scores of gymnasts on multiple apparatus types can be represented as a data frame whose rows are indexed by the names of the gymnasts and whole columns are indexed by the names of the apparatuses. 
::::

For example, here is a pandas series for the wavelengths of light corresponding to rainbow colors:

```{python}
import pandas as pd

wavelength = pd.Series( 
  [400, 470, 520, 580, 610, 710],    # values
  index=["violet", "blue", "green", "yellow", "orange", "red"]
  )

print(wavelength)
```

We can now use an index value to access a value in the series.

```{python}
print( wavelength["blue"] )
```

We can access multiple values to get a series that is a subset of the original.

```{python}
print( wavelength[ ["violet", "red"] ] )
```

We can also use the `iloc` property to access the underlying vector in NumPy fashion.

```{python}
wavelength.iloc[:4]
```

Here is a series of NFL teams based on the same index.

```{python}
teams = pd.Series(
  ["Vikings", "Bills", "Eagles", "Chargers", "Bengals", "Cardinals"],
  index = wavelength.index
  )

teams["green"]
```

Now we can create a data frame using these two series as columns.

```{python}
rainbow = pd.DataFrame( {"wavelength":wavelength, "team name":teams} )
rainbow
```

We can access a column using simple bracket notation:

```{python}
rainbow["team name"]
```

We can add a column after the fact using a bracket access on the left side of the assignment:

```{python}
rainbow["flower"] = ["Lobelia", "Cornflower", "Bells-of-Ireland", "Daffodil"," Butterfly weed"," Rose"]
rainbow
```

We can access a row by using brackets with the `loc` property of the frame:

```{python}
rainbow.loc["orange"]
```

The result above is a series indexed by the column names of the frame. We are also free to strip away the index list and get an ordinary array:

```{python}
rainbow.loc["red"].to_numpy()
```

Here is another way to construct a data frame. We give a vector of rows, plus (optionally) the index and the names of the columns.

```{python}
letters = pd.DataFrame( 
    [ ("a", "A"), ("b", "B"), ("c", "C") ], 
    columns=["lowercase", "uppercase"] 
    )

letters
```

### Categorical data

Pandas has facilities for dealing with categorical variables. For example, here is a vector of chess pieces at the start of a game:

```{python}
pieces = np.hstack( [
    np.repeat("pawn", 8),
    np.repeat(["knight","bishop","rook"], 2),
    "queen",
    "king"
] )

pieces
```

We can tell pandas to regard these strings as elements of a category set:

```{python}
pd.Categorical(pieces)
```

Notice above how the unique categories were found automatically. 

For much more about pandas fundamentals, try the [Kaggle course](https://www.kaggle.com/learn/pandas).

## Data preparation

Raw data often needs to be manipulated into a useable format before algorithms can be applied. Preprocessing data so that it is suitable for machine analysis is known as **data wrangling** or **data munging**. A related process is **data cleaning**, where missing and anomalous values are removed or replaced.

### Missing values

In real data sets, we often must cope with data series that have missing values. This is a common source of mistakes and confusion, especially because there is no universal practice. Sometimes zero is used to represent a missing number. It's also common to use an impossible value, such as $-999$ to represent weight, to signify missing data. 

Formally, the most natural way to represent missing data in Python is as `nan` or `NaN`, and pandas makes it easy to find and manipulate such values. Here is another well-known data set, this time about penguins:

```{python}
import seaborn as sns
penguins = sns.load_dataset("penguins")
penguins.head()
```

Note above that the fourth row of the frame is missing measurements. We can discover how many such rows there are using `isna`:

```{python}
penguins.isna().sum()
```

Sometimes one replaces missing values with average or other representative values, a process called *imputation*. It's often prudent to simply toss them out, as follows:

```{python}
print("original counts:")
print( penguins.count() )
penguins.dropna( inplace=True )
print()
print("after removals:")
print( penguins.count() )
```

::: {.callout-tip}
Operations that make changes to or retrieve subsets from a data frame work on copies of the frame. When `inplace=True` is given, though, the operation changes the original frame.
:::

### Loans example

To demonstrate algortihms in later sections, we will be using a [dataset describing loans](https://www.kaggle.com/datasets/imsparsh/lending-club-loan-dataset-2007-2011) made on the crowdfunding site LendingClub. First, we load the raw data from a CSV (comma separated values) file.

:::{.callout-tip}
It's possible to import datasets from the Web directly into pandas. However, web sources and links change and disappear frequently, so if storing the dataset is not a problem, you may want to download your own copy before working on it.
:::

```{python}
import pandas as pd
loans = pd.read_csv("loan.csv")
loans.head()
```

The `int_rate` column, which gives the interest rate on the loan, has been interpreted as strings due to the percent sign. We'll strip out those percent signs and convert them to floats.

:::{.callout-tip}
As you see below, we often end up with chains of methods separated by dots. Python works from left to right, evaluating a subexpression and then replacing it with the object for the next segment in the chain. We could write these as a sequence of separate lines having intermediate results assigned to variable names, but it's often considered better style to chain them.
:::

```{python}
loans["int_rate"] = loans["int_rate"].str.strip('%').astype(float)
loans.head()
```

Let's add a column for the percentage of the loan request that was eventually funded. This will be a target for some of our learning methods.

```{python}
loans["percent_funded"] = 100 * loans["funded_amnt"] / loans["loan_amnt"]
target = ["percent_funded"]
```

We will only use a small subset of the numerical columns as features. Let's verify that there are no missing values in those columns.

```{python}
features = [ "loan_amnt", "int_rate", "installment", "annual_inc",
    "dti", "delinq_2yrs", "delinq_amnt" ]
loans = loans.loc[:, features + target]
loans.isna().sum()
```

:::{.callout-note}
Given lists of columns names (or any strings), you can use `+` to concatenate them into a single list.
:::

Finally, we'll output this cleaned data frame to its own CSV file. The index row is an ID number that is meaningless to classification, so we will instruct pandas to exclude it from the new file:

```{python}
loans.to_csv("loan_clean.csv", index=False)
```

### Diamonds example

We will also be using a seaborn [dataset comprising features of diamonds and their prices](https://ggplot2.tidyverse.org/reference/diamonds.html):

```{python}
import seaborn as sns
diamonds = sns.load_dataset("diamonds")
diamonds.head()
```

As you can see above, some of the features (cut, color, clarity) have string designations. However, these do have a specific ordering in this context. So we will replace the strings with the correct ordinal values.

We start with the *cut* column:

```{python}
cuts = ["Fair", "Good", "Very Good", "Premium", "Ideal"]
diamonds["cut"].replace(
    cuts,           # to be replaced
    range(5),       # replacements
    inplace=True    # change the original frame, not a copy
    )

diamonds.head()
```

Above, you see that the *cut* strings have been replaced by integers. Now we do the same for the other categories:

```{python}
diamonds["clarity"].replace(
    ["I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"],
    range(8),
    inplace=True
    )

diamonds["color"].replace(
    list("DEFGHIJ"),
    range(7),
    inplace=True
    )

diamonds.head()
```

We'll save this modified dataset to its own file for our future use:

```{python}
diamonds.to_csv("diamonds.csv", index=False)
```